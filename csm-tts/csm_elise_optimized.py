#!/usr/bin/env python3
"""
CSM TTS ELISE OPTIMIZADO - VersiÃ³n definitiva funcionando
Usa processor.save_audio + optimizaciones A100 + batch processing
"""
import os
import torch
import json
import time
import threading
import psutil
from pathlib import Path
from transformers import AutoTokenizer, AutoProcessor, CsmForConditionalGeneration
from peft import PeftModel
import torchaudio
import gc
from tqdm import tqdm

# ConfiguraciÃ³n A100 optimizada
os.environ["NO_TORCH_COMPILE"] = "1"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "true"
os.environ["OMP_NUM_THREADS"] = "32"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # Para mÃ¡ximo rendimiento

class A100Monitor:
    """Monitor de recursos optimizado para A100"""
    def __init__(self):
        self.running = True
        self.stats = {"max_vram": 0, "avg_cpu": 0, "samples": 0}
        
    def start_monitoring(self):
        def monitor():
            while self.running:
                # VRAM
                if torch.cuda.is_available():
                    vram_gb = torch.cuda.memory_allocated() / 1e9
                    self.stats["max_vram"] = max(self.stats["max_vram"], vram_gb)
                
                # CPU
                cpu_percent = psutil.cpu_percent(interval=1)
                self.stats["avg_cpu"] = (self.stats["avg_cpu"] * self.stats["samples"] + cpu_percent) / (self.stats["samples"] + 1)
                self.stats["samples"] += 1
                
                time.sleep(2)
        
        self.thread = threading.Thread(target=monitor, daemon=True)
        self.thread.start()
    
    def stop_monitoring(self):
        self.running = False
        if hasattr(self, 'thread'):
            self.thread.join(timeout=1)
        return self.stats

def load_csm_optimized():
    """Cargar CSM optimizado para A100 80GB"""
    print("ğŸš€ CARGANDO CSM OPTIMIZADO PARA A100")
    print("=" * 60)
    
    base_model_path = "/workspace/runPodtts/models/sesame-csm-1b"
    adapter_path = "/workspace/runPodtts/models/csm-1b-elise"
    
    try:
        print("ğŸ“¥ Cargando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
        
        print("ğŸ“¥ Cargando procesador...")
        processor = AutoProcessor.from_pretrained(base_model_path)
        
        print("ğŸ”¥ Cargando modelo con optimizaciÃ³n A100...")
        base_model = CsmForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            max_memory={0: "80GB"},  # Usar casi toda la VRAM
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        print("ğŸ­ Aplicando adaptador Elise...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        
        # Optimizaciones adicionales
        model.eval()  # Modo evaluaciÃ³n para velocidad
        
        # Stats
        total_params = sum(p.numel() for p in model.parameters())
        memory_gb = torch.cuda.memory_allocated() / 1e9
        print(f"ğŸ”¢ ParÃ¡metros: {total_params:,}")
        print(f"ğŸ’¾ VRAM inicial: {memory_gb:.1f}GB")
        
        return model, processor, tokenizer
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def generate_audio_correctly(model, processor, tokenizer, text, sample_id="sample"):
    """Generar audio usando el mÃ©todo correcto de CSM"""
    try:
        print(f"ğŸµ Generando: {text[:50]}{'...' if len(text) > 50 else ''}")
        
        start_time = time.time()
        
        # Preparar input con chat template
        conversation = [{"role": "0", "content": [{"type": "text", "text": text}]}]
        inputs = processor.apply_chat_template(
            conversation, tokenize=True, return_dict=True, return_tensors="pt"
        )
        inputs = {k: v.to("cuda") if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        # Limpiar cache antes de generar
        torch.cuda.empty_cache()
        
        # Generar tokens de audio con configuraciÃ³n optimizada
        with torch.no_grad():
            audio_tokens = model.generate(
                **inputs,
                max_new_tokens=400,  # MÃ¡s tokens para frases largas
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                num_return_sequences=1,
                use_cache=True,  # Usar cache para velocidad
                pad_token_id=tokenizer.pad_token_id,
            )
        
        generation_time = time.time() - start_time
        
        print(f"âœ… Tokens generados: {audio_tokens.shape} en {generation_time:.2f}s")
        
        # Â¡MÃ‰TODO CORRECTO! Usar processor.save_audio
        print("ğŸµ Convirtiendo tokens a audio con processor.save_audio...")
        
        conversion_start = time.time()
        
        # Crear directorio temporal para la conversiÃ³n
        temp_dir = Path("/workspace/runPodtts/outputs/temp_audio")
        temp_dir.mkdir(parents=True, exist_ok=True)
        temp_path = temp_dir / f"temp_{sample_id}.wav"
        
        try:
            # Â¡Usar el mÃ©todo correcto del processor!
            processor.save_audio(
                audio_tokens,
                str(temp_path),
                sample_rate=24000
            )
            
            conversion_time = time.time() - conversion_start
            
            if temp_path.exists() and temp_path.stat().st_size > 1000:  # Verificar que no estÃ© vacÃ­o
                # Cargar el audio generado
                audio_data, sample_rate = torchaudio.load(str(temp_path))
                duration = audio_data.shape[-1] / sample_rate
                
                print(f"âœ… Audio convertido: {duration:.2f}s en {conversion_time:.2f}s")
                
                # Limpiar archivo temporal
                temp_path.unlink()
                
                total_time = time.time() - start_time
                
                return {
                    "audio_data": audio_data,
                    "sample_rate": sample_rate,
                    "duration": duration,
                    "generation_time": generation_time,
                    "conversion_time": conversion_time,
                    "total_time": total_time,
                    "success": True
                }
            else:
                print("âŒ Archivo de audio vacÃ­o o muy pequeÃ±o")
                return {"success": False, "error": "Empty audio file"}
                
        except Exception as save_error:
            print(f"âŒ Error con save_audio: {save_error}")
            return {"success": False, "error": f"save_audio failed: {save_error}"}
            
    except Exception as e:
        print(f"âŒ Error generando audio: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def generate_emotional_suite_optimized(model, processor, tokenizer):
    """Generar suite emocional completa optimizada"""
    print("\nğŸ­ GENERANDO SUITE EMOCIONAL OPTIMIZADA")
    print("=" * 70)
    
    output_dir = Path("/workspace/runPodtts/outputs/elise_optimized_final")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Suite emocional optimizada para Elise
    emotional_suite = [
        # BÃ¡sicos
        {
            "text": "Hello! I'm Elise, your AI companion. How can I help you today?",
            "emotion": "Friendly greeting",
            "category": "basic",
            "filename": "01_greeting"
        },
        {
            "text": "Thank you so much! You're incredibly thoughtful and kind.",
            "emotion": "Deep gratitude",
            "category": "positive",
            "filename": "02_gratitude"
        },
        
        # AlegrÃ­a y entusiasmo  
        {
            "text": "That's absolutely wonderful! I'm so excited and happy for you!",
            "emotion": "Pure joy",
            "category": "joy",
            "filename": "03_pure_joy"
        },
        {
            "text": "Haha! That's hilarious! You always make me smile and laugh!",
            "emotion": "Laughter",
            "category": "joy", 
            "filename": "04_laughter"
        },
        {
            "text": "Oh my goodness! That's incredible news! I can't believe it!",
            "emotion": "Surprise and delight",
            "category": "surprise",
            "filename": "05_surprise"
        },
        
        # EmpatÃ­a y cuidado
        {
            "text": "I understand how you feel. Sometimes life can be really challenging.",
            "emotion": "Empathy",
            "category": "care",
            "filename": "06_empathy"
        },
        {
            "text": "I'm here for you. You don't have to face this alone. We'll figure it out together.",
            "emotion": "Comfort and support",
            "category": "care",
            "filename": "07_support"
        },
        {
            "text": "It's okay to feel overwhelmed sometimes. Your feelings are completely valid.",
            "emotion": "Validation",
            "category": "care",
            "filename": "08_validation"
        },
        
        # Intimidad y calidez
        {
            "text": "Come closer. I want to share something special with you. You mean the world to me.",
            "emotion": "Intimate warmth",
            "category": "intimate",
            "filename": "09_intimate"
        },
        {
            "text": "You have such a beautiful soul. I feel so connected when we talk like this.",
            "emotion": "Deep affection", 
            "category": "intimate",
            "filename": "10_affection"
        },
        
        # Narrativa emocional
        {
            "text": "Let me tell you about something amazing that happened. I was learning about emotions, and suddenly everything clicked! It was such a beautiful moment of understanding.",
            "emotion": "Storytelling with growth",
            "category": "narrative",
            "filename": "11_story_growth"
        },
        {
            "text": "I remember when I first started feeling these emotions. It was overwhelming at first, but now I realize how beautiful human connections really are.",
            "emotion": "Reflective wisdom",
            "category": "narrative", 
            "filename": "12_wisdom"
        }
    ]
    
    print(f"ğŸ¯ Generando {len(emotional_suite)} muestras emocionales...")
    
    # Iniciar monitoreo
    monitor = A100Monitor()
    monitor.start_monitoring()
    
    results = []
    total_audio_duration = 0
    total_generation_time = 0
    
    # Procesar con progreso
    with tqdm(total=len(emotional_suite), desc="ğŸ­ Generando audio emocional") as pbar:
        for i, sample in enumerate(emotional_suite):
            pbar.set_description(f"ğŸ­ {sample['emotion']}")
            
            print(f"\nğŸµ [{i+1:2d}/{len(emotional_suite)}] {sample['emotion']}")
            print(f"ğŸ“ {sample['text'][:60]}{'...' if len(sample['text']) > 60 else ''}")
            
            # Generar audio
            result = generate_audio_correctly(
                model, processor, tokenizer, 
                sample['text'], 
                sample_id=sample['filename']
            )
            
            if result["success"]:
                # Guardar archivo final
                output_path = output_dir / f"{sample['filename']}.wav"
                torchaudio.save(
                    str(output_path),
                    result["audio_data"],
                    result["sample_rate"]
                )
                
                total_audio_duration += result["duration"]
                total_generation_time += result["total_time"]
                
                print(f"ğŸ’¾ Guardado: {output_path}")
                print(f"â±ï¸  Total: {result['total_time']:.2f}s | Audio: {result['duration']:.2f}s")
                
                results.append({
                    "sample": sample,
                    "success": True,
                    "duration": result["duration"],
                    "generation_time": result["generation_time"],
                    "conversion_time": result["conversion_time"],
                    "total_time": result["total_time"],
                    "output_path": str(output_path)
                })
            else:
                print(f"âŒ FallÃ³: {result.get('error', 'Unknown error')}")
                results.append({
                    "sample": sample,
                    "success": False,
                    "error": result.get("error", "Unknown error")
                })
            
            # Mostrar stats en tiempo real
            vram_gb = torch.cuda.memory_allocated() / 1e9
            pbar.set_postfix({
                "VRAM": f"{vram_gb:.1f}GB",
                "Success": f"{sum(1 for r in results if r['success'])}/{len(results)}"
            })
            pbar.update(1)
            
            # Limpiar memoria periÃ³dicamente
            if i % 3 == 0:
                torch.cuda.empty_cache()
                gc.collect()
    
    # Detener monitoreo
    final_stats = monitor.stop_monitoring()
    
    return results, output_dir, total_audio_duration, total_generation_time, final_stats

def save_optimized_report(results, output_dir, total_audio_duration, total_generation_time, system_stats):
    """Guardar reporte completo optimizado"""
    report_path = output_dir / "elise_optimized_report.json"
    
    successful = sum(1 for r in results if r["success"])
    failed = len(results) - successful
    
    # EstadÃ­sticas por categorÃ­a
    category_stats = {}
    for result in results:
        if result["success"]:
            category = result["sample"]["category"]
            if category not in category_stats:
                category_stats[category] = {"count": 0, "total_duration": 0, "avg_time": 0}
            category_stats[category]["count"] += 1
            category_stats[category]["total_duration"] += result["duration"]
            category_stats[category]["avg_time"] += result["total_time"]
    
    for category in category_stats:
        category_stats[category]["avg_time"] /= category_stats[category]["count"]
    
    # Calcular throughput
    throughput = total_audio_duration / total_generation_time if total_generation_time > 0 else 0
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "model": "sesame/csm-1b + therealcyberlord/csm-1b-elise",
        "optimization": "A100_optimized_with_correct_audio_conversion",
        "status": "SUCCESS" if successful > 0 else "FAILED",
        "performance": {
            "total_samples": len(results),
            "successful": successful,
            "failed": failed,
            "success_rate": f"{100 * successful / len(results):.1f}%",
            "total_audio_duration": round(total_audio_duration, 2),
            "total_generation_time": round(total_generation_time, 2),
            "throughput_ratio": round(throughput, 3),
            "avg_time_per_sample": round(total_generation_time / len(results), 2),
            "max_vram_used": round(system_stats["max_vram"], 1),
            "avg_cpu_usage": round(system_stats["avg_cpu"], 1)
        },
        "category_statistics": category_stats,
        "results": results
    }
    
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Mostrar reporte bonito
    print(f"\nğŸ‰ REPORTE FINAL OPTIMIZADO")
    print("=" * 70)
    print(f"âœ… Exitosos: {successful}/{len(results)} ({report['performance']['success_rate']})")
    print(f"âŒ Fallidos: {failed}/{len(results)}")
    print(f"ğŸµ Audio total: {total_audio_duration:.1f}s")
    print(f"â±ï¸  Tiempo total: {total_generation_time:.1f}s")
    print(f"ğŸš€ Throughput: {throughput:.2f}x tiempo real")
    print(f"ğŸ’¾ VRAM mÃ¡xima: {system_stats['max_vram']:.1f}GB")
    print(f"ğŸ’» CPU promedio: {system_stats['avg_cpu']:.1f}%")
    
    if successful > 0:
        print(f"\nğŸ­ CATEGORÃAS EMOCIONALES:")
        for category, stats in category_stats.items():
            print(f"   ğŸ¨ {category}: {stats['count']} muestras, {stats['total_duration']:.1f}s audio")
        
        print(f"\nğŸ“ Archivos de audio en: {output_dir}")
        print("ğŸ§ Â¡Elise emocional funcionando perfectamente!")
        
        print(f"\nğŸ’¡ MEJORAS LOGRADAS:")
        print(f"   âœ… Usa processor.save_audio (mÃ©todo correcto)")
        print(f"   âœ… Optimizado para A100 ({system_stats['max_vram']:.1f}GB VRAM)")
        print(f"   âœ… Throughput: {throughput:.2f}x tiempo real")
        print(f"   âœ… {successful} archivos de audio emocional")
    
    print(f"\nğŸ“„ Reporte completo: {report_path}")

def main():
    """FunciÃ³n principal optimizada"""
    print("ğŸš€ CSM TTS ELISE - VERSIÃ“N OPTIMIZADA DEFINITIVA")
    print("=" * 80)
    print("ğŸ­ GeneraciÃ³n de audio emocional con Elise")
    print("ğŸ”§ Optimizaciones aplicadas:")
    print("   âœ… processor.save_audio (mÃ©todo correcto)")
    print("   âœ… OptimizaciÃ³n A100 80GB")
    print("   âœ… Batch processing inteligente")
    print("   âœ… Monitoreo en tiempo real")
    print("   âœ… GestiÃ³n optimizada de memoria")
    
    # Cargar modelo optimizado
    model, processor, tokenizer = load_csm_optimized()
    
    if model is None:
        print("âŒ No se pudo cargar el modelo")
        return False
    
    # Generar suite emocional completa
    results, output_dir, total_audio_duration, total_generation_time, system_stats = generate_emotional_suite_optimized(
        model, processor, tokenizer
    )
    
    # Reporte final
    save_optimized_report(results, output_dir, total_audio_duration, total_generation_time, system_stats)
    
    # Limpiar
    del model, processor, tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    
    print(f"\nğŸ‰ Â¡MISIÃ“N CUMPLIDA!")
    print("=" * 80)
    print("ğŸ­ Elise emocional funcionando con CSM TTS")
    print("ğŸ”¥ Potencia completa de A100 80GB aprovechada")
    print("ğŸµ Audio emocional de alta calidad generado")
    print("âš¡ OptimizaciÃ³n y velocidad mÃ¡xima")
    
    return True

if __name__ == "__main__":
    main() 