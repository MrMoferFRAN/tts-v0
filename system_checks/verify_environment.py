#!/usr/bin/env python3
"""
ğŸ” Script de VerificaciÃ³n Completa del Entorno RunPod
Verifica todas las capacidades del sistema para CSM TTS + Elise
"""

import subprocess
import sys
import torch
import psutil
import platform
from pathlib import Path

def print_section(title):
    """Imprimir secciÃ³n con formato"""
    print(f"\n{'='*50}")
    print(f"ğŸ” {title}")
    print('='*50)

def run_command(cmd):
    """Ejecutar comando y retornar output"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip()
    except Exception as e:
        return f"Error: {e}"

def check_gpu_capabilities():
    """Verificar capacidades detalladas de GPU"""
    print_section("VERIFICACIÃ“N DE GPU")
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA disponible: {torch.cuda.is_available()}")
        print(f"âœ… VersiÃ³n CUDA: {torch.version.cuda}")
        print(f"âœ… NÃºmero de GPUs: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_props = torch.cuda.get_device_properties(i)
            print(f"\nğŸ”¥ GPU {i}: {gpu_props.name}")
            print(f"   ğŸ“Š Memoria total: {gpu_props.total_memory / 1e9:.1f} GB")
            print(f"   ğŸ”§ Compute capability: {gpu_props.major}.{gpu_props.minor}")
            print(f"   ğŸ­ Multiprocessors: {gpu_props.multi_processor_count}")
            
        # Test de memoria GPU
        print(f"\nğŸ§ª Test de memoria GPU:")
        device = torch.device('cuda:0')
        try:
            # Crear tensor de prueba
            test_tensor = torch.randn(1000, 1000, device=device)
            memory_allocated = torch.cuda.memory_allocated(device) / 1e9
            memory_reserved = torch.cuda.memory_reserved(device) / 1e9
            print(f"   âœ… Memoria asignada: {memory_allocated:.2f} GB")
            print(f"   âœ… Memoria reservada: {memory_reserved:.2f} GB")
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"   âŒ Error en test de GPU: {e}")
    else:
        print("âŒ CUDA no disponible")

def check_system_resources():
    """Verificar recursos del sistema"""
    print_section("RECURSOS DEL SISTEMA")
    
    # CPU
    print(f"ğŸ–¥ï¸  CPU: {platform.processor()}")
    print(f"ğŸ”¢ Cores: {psutil.cpu_count(logical=False)} fÃ­sicos, {psutil.cpu_count(logical=True)} lÃ³gicos")
    print(f"ğŸ“Š Uso actual CPU: {psutil.cpu_percent(interval=1)}%")
    
    # Memoria
    memory = psutil.virtual_memory()
    print(f"ğŸ§  RAM Total: {memory.total / 1e9:.1f} GB")
    print(f"ğŸ§  RAM Disponible: {memory.available / 1e9:.1f} GB")
    print(f"ğŸ§  RAM Usado: {memory.percent}%")
    
    # Disco
    disk = psutil.disk_usage('/workspace')
    print(f"ğŸ’¾ Disco Total: {disk.total / 1e12:.1f} TB")
    print(f"ğŸ’¾ Disco Disponible: {disk.free / 1e12:.1f} TB")
    print(f"ğŸ’¾ Disco Usado: {(disk.used/disk.total)*100:.1f}%")

def check_python_environment():
    """Verificar entorno Python"""
    print_section("ENTORNO PYTHON")
    
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ“¦ PyTorch: {torch.__version__}")
    print(f"ğŸ“ PyTorch path: {torch.__file__}")
    
    # Verificar paquetes importantes
    packages_to_check = [
        'numpy', 'scipy', 'matplotlib', 'librosa', 
        'transformers', 'datasets', 'tqdm', 'wandb'
    ]
    
    print("\nğŸ“š Paquetes importantes:")
    for package in packages_to_check:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'Unknown')
            print(f"   âœ… {package}: {version}")
        except ImportError:
            print(f"   âŒ {package}: No instalado")

def check_audio_capabilities():
    """Verificar capacidades de audio"""
    print_section("CAPACIDADES DE AUDIO")
    
    # FFmpeg
    ffmpeg_version = run_command("ffmpeg -version | head -1")
    if "ffmpeg version" in ffmpeg_version:
        print(f"âœ… FFmpeg: {ffmpeg_version}")
    else:
        print("âŒ FFmpeg no encontrado")
    
    # Add SoX check
    sox_version = run_command("sox --version | head -1")
    if "SoX" in sox_version:
        print(f"âœ… SoX: {sox_version}")
    else:
        print("âŒ SoX no encontrado")
    
    # Verificar librosa
    try:
        import librosa
        print(f"âœ… Librosa: {librosa.__version__}")
        
        # Test bÃ¡sico de librosa
        import numpy as np
        test_audio = np.random.randn(22050)  # 1 segundo de audio
        mfcc = librosa.feature.mfcc(y=test_audio, sr=22050, n_mfcc=13)
        print(f"âœ… Test MFCC: {mfcc.shape}")
    except Exception as e:
        print(f"âŒ Error con librosa: {e}")

def estimate_training_capacity():
    """Estimar capacidad de entrenamiento"""
    print_section("ESTIMACIÃ“N DE CAPACIDAD DE ENTRENAMIENTO")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"ğŸ’ª Con {gpu_memory:.0f}GB de VRAM puedes entrenar:")
        
        if gpu_memory >= 80:
            print("   ğŸ”¥ Modelos GRANDES (>1B parÃ¡metros) - Full finetuning")
            print("   ğŸ”¥ Batch size: 32-64+")
            print("   ğŸ”¥ MÃºltiples experimentos simultÃ¡neos")
            print("   ğŸ”¥ Sin necesidad de tÃ©cnicas de optimizaciÃ³n")
        elif gpu_memory >= 40:
            print("   âœ… Modelos MEDIANOS-GRANDES - Full finetuning")
            print("   âœ… Batch size: 16-32")
            print("   âœ… La mayorÃ­a de modelos sin problemas")
        elif gpu_memory >= 24:
            print("   âœ… Modelos MEDIANOS - Full finetuning")
            print("   âœ… Batch size: 8-16")
            print("   âš ï¸  Modelos grandes requieren LoRA")
        else:
            print("   âš ï¸  Solo modelos PEQUEÃ‘OS o tÃ©cnicas optimizadas")
            print("   âš ï¸  Batch size: 4-8")
            print("   âš ï¸  Requiere LoRA/QLoRA para modelos grandes")
    
    # EstimaciÃ³n de tiempo y costo
    print(f"\nâ±ï¸  Estimaciones para dataset Elise:")
    print(f"   ğŸ‹ï¸  Full finetuning: 6-12 horas")
    print(f"   âš¡ LoRA finetuning: 3-6 horas")
    print(f"   ğŸš€ Con A100 80GB: Sin limitaciones de memoria")

def main():
    """FunciÃ³n principal"""
    print("ğŸ¯ VERIFICACIÃ“N COMPLETA DEL ENTORNO RUNPOD")
    print("Para CSM TTS + Elise Finetuning")
    print(f"Timestamp: {subprocess.run(['date'], capture_output=True, text=True).stdout.strip()}")
    
    check_gpu_capabilities()
    check_system_resources()
    check_python_environment()
    check_audio_capabilities()
    estimate_training_capacity()
    
    print_section("RESUMEN FINAL")
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ¯ SISTEMA LISTO para CSM TTS")
        print(f"ğŸ”¥ GPU: {gpu_name} ({gpu_memory:.0f}GB)")
        print(f"âœ… Puedes proceder con el setup de CSM")
        print(f"âœ… Entrenamiento de Elise SIN limitaciones de memoria")
    else:
        print("âŒ SISTEMA NO LISTO - CUDA no disponible")
    
    print("\nğŸš€ Siguiente paso: ejecutar setup_csm_runpod.sh")

if __name__ == "__main__":
    main() 